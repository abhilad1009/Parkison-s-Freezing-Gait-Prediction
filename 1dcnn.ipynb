{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-09T10:29:35.854623Z","iopub.status.busy":"2023-06-09T10:29:35.854208Z","iopub.status.idle":"2023-06-09T10:29:35.861672Z","shell.execute_reply":"2023-06-09T10:29:35.859727Z","shell.execute_reply.started":"2023-06-09T10:29:35.854594Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import os\n","import pandas as pd\n","import numpy as np\n","import itertools\n","from sklearn.model_selection import train_test_split\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","from torch import nn\n","from torch.nn import functional as F"]},{"cell_type":"markdown","metadata":{},"source":["Create custom data loader to segment sensor time series in 256 length chuncks"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-06-09T10:29:35.881478Z","iopub.status.busy":"2023-06-09T10:29:35.880513Z","iopub.status.idle":"2023-06-09T10:29:35.906627Z","shell.execute_reply":"2023-06-09T10:29:35.905166Z","shell.execute_reply.started":"2023-06-09T10:29:35.881436Z"},"trusted":true},"outputs":[],"source":["class SegmentedDataset(Dataset):\n","    def __init__(self, folder_path, files, is_train=True, segment_length=256):\n","        self.folder_path = folder_path\n","        self.is_train = is_train\n","        self.segment_length = segment_length\n","        self.data, self.time, self.ids, self.targets = self.load_data(files)\n","\n","    def load_data(self, files):\n","        data, time, ids, targets = [], [], [], []\n","\n","        for file_name in files:\n","            file_path = os.path.join(self.folder_path, file_name)\n","            file_id = file_name.replace('.csv', '')\n","\n","            data = pd.read_csv(file_path)\n","            time = data['Time'].values\n","            features = data[['AccV', 'AccML', 'AccAP']].values\n","\n","            if self.is_train:\n","                targets = data[['StartHesitation', 'Turn', 'Walking']].values\n","                targets = np.concatenate([targets, (1 - targets.sum(axis=1)).reshape(-1, 1)], axis=1)\n","\n","            num_segments = len(features) // self.segment_length\n","            remainder = len(features) % self.segment_length\n","\n","            for i in range(num_segments):\n","                feature_segment = features[i * self.segment_length:(i + 1) * self.segment_length]\n","                time_segment = time[i * self.segment_length:(i + 1) * self.segment_length]\n","                target_segment = targets[i * self.segment_length:(i + 1) * self.segment_length] if self.is_train else None\n","\n","                self.append_to_lists(feature_segment, time_segment, target_segment, file_id)\n","\n","            if remainder > 0:\n","                padding_length = self.segment_length - remainder\n","                feature_segment = np.pad(features[-remainder:], ((0, padding_length), (0, 0)), mode='constant')\n","                time_segment = np.pad(time[-remainder:], (0, padding_length), mode='constant', constant_values=-1)\n","                target_segment = np.pad(targets[-remainder:], ((0, padding_length), (0, 0)), mode='constant') if self.is_train else None\n","\n","                self.append_to_lists(feature_segment, time_segment, target_segment, file_id)\n","\n","        return data, time, ids, targets\n","\n","    def append_to_lists(self, feature_segment, time_segment, target_segment, file_id):\n","        self.data.append(feature_segment)\n","        self.time.append(time_segment)\n","        self.ids.append(file_id)\n","\n","        if self.is_train:\n","            self.targets.append(target_segment)\n","\n","    def get_segment_indices(self):\n","        return self.segment_indices\n","\n","    def get_segment_padding(self):\n","        return self.segment_padding\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        if self.is_train:\n","            return torch.Tensor(self.data[idx]), torch.Tensor(np.argmax(self.targets[idx], axis=1)).to(torch.int64)\n","        else:\n","            return torch.Tensor(self.data[idx]), self.ids[idx], self.time[idx]\n"]},{"cell_type":"markdown","metadata":{},"source":["Define model with 1D Conv block and training routine"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-09T10:29:35.911694Z","iopub.status.busy":"2023-06-09T10:29:35.91004Z","iopub.status.idle":"2023-06-09T10:29:35.941559Z","shell.execute_reply":"2023-06-09T10:29:35.939923Z","shell.execute_reply.started":"2023-06-09T10:29:35.911579Z"},"trusted":true},"outputs":[],"source":["class ConvBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_sizes, dropout_rate, dilations):\n","        super(ConvBlock, self).__init__()\n","\n","        self.convs = nn.ModuleList([\n","            nn.Conv1d(in_channels, out_channels, kernel_size, padding=(kernel_size + (kernel_size - 1) * (dilation - 1)) // 2, dilation=dilation)\n","            for kernel_size, dilation in itertools.product(kernel_sizes, dilations)\n","        ])\n","\n","        self.batch_norm = nn.BatchNorm1d(out_channels * len(kernel_sizes) * len(dilations))\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","        self.skip_connection = nn.Conv1d(in_channels, out_channels * len(kernel_sizes) * len(dilations), kernel_size=1) \\\n","            if in_channels != out_channels * len(kernel_sizes) * len(dilations) else nn.Identity()\n","\n","    def forward(self, x):\n","        x = x.transpose(1, 2)\n","        skip = self.skip_connection(x)\n","        x = torch.cat([conv(x) for conv in self.convs], dim=1)\n","        x = self.batch_norm(x + skip)\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        x = x.transpose(1, 2)\n","        return x\n","\n","class Model(pl.LightningModule):\n","    def __init__(self, in_channels, out_channels, kernel_sizes, dilations, dropout_rate, num_blocks, lr=0.001):\n","        super(Model, self).__init__()\n","        self.train_loss_history = []\n","        self.val_loss_history = []\n","        self.val_score_history = []\n","        self.lr = lr\n","        self.blocks = nn.Sequential(*[\n","            ConvBlock(in_channels if i == 0 else out_channels * len(kernel_sizes) * len(dilations),\n","                      out_channels,\n","                      kernel_sizes,\n","                      dropout_rate,\n","                      dilations)\n","            for i in range(num_blocks)\n","        ])\n","\n","        num_classes = 4\n","        self.linear = nn.Linear(out_channels * len(kernel_sizes) * len(dilations), num_classes)\n","        weights = torch.tensor([1.0, 1.0, 1.0, 0.1])\n","        self.loss = torch.nn.NLLLoss(weight=weights)\n","        self.logsoftmax = nn.LogSoftmax(dim=2)\n","        self.output = []\n","\n","    def forward(self, x):\n","        x = self.blocks(x)\n","        x = self.linear(x)\n","        return x\n","\n","    def training_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        y_hat = self.logsoftmax(y_hat)\n","        loss = self.loss(y_hat.view(-1, y_hat.size(-1)), y.view(-1))\n","        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n","        self.train_loss_history.append(loss.item())\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        x, y = batch\n","        y_hat = self(x)\n","        loss = self.loss(y_hat.view(-1, y_hat.size(-1)), y.view(-1))\n","        self.log('val_loss', loss)\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        x, file_id, time = batch\n","        y_hat = self.forward(x)\n","        y_hat = F.softmax(y_hat, dim=2)\n","        self.output.append((y_hat, file_id, time))\n","\n","    def configure_optimizers(self):\n","        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n","        return optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-09T10:29:35.944011Z","iopub.status.busy":"2023-06-09T10:29:35.943324Z","iopub.status.idle":"2023-06-09T10:29:43.088297Z","shell.execute_reply":"2023-06-09T10:29:43.085956Z","shell.execute_reply.started":"2023-06-09T10:29:35.943974Z"},"trusted":true},"outputs":[],"source":["folder_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/train/tdcsfog/'\n","all_files = [file for file in os.listdir(folder_path)]\n","train_files, val_files = train_test_split(all_files, random_state=13)\n","\n","\n","train_dataset = SegmentedDataset(folder_path, train_files)\n","val_dataset = SegmentedDataset(folder_path, val_files)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64)\n","val_loader = DataLoader(val_dataset, batch_size=64)"]},{"cell_type":"markdown","metadata":{},"source":["Train the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-06-09T10:29:43.089554Z","iopub.status.idle":"2023-06-09T10:29:43.089992Z","shell.execute_reply":"2023-06-09T10:29:43.089801Z","shell.execute_reply.started":"2023-06-09T10:29:43.089782Z"},"trusted":true},"outputs":[],"source":["model = Model(\n","    in_channels=3,\n","    out_channels=3,\n","    kernel_sizes=[3,5,7],\n","    dilations=[2,4,8],\n","    dropout_rate=0.1,\n","    num_blocks=1,\n",")\n","\n","trainer = pl.Trainer(max_epochs=10)\n","trainer.fit(model, train_loader, val_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-06-09T10:29:43.092894Z","iopub.status.idle":"2023-06-09T10:29:43.093637Z","shell.execute_reply":"2023-06-09T10:29:43.093327Z","shell.execute_reply.started":"2023-06-09T10:29:43.093295Z"},"trusted":true},"outputs":[],"source":["submission = pd.read_csv(\"/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/sample_submission.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-06-09T10:29:43.095933Z","iopub.status.idle":"2023-06-09T10:29:43.096632Z","shell.execute_reply":"2023-06-09T10:29:43.096344Z","shell.execute_reply.started":"2023-06-09T10:29:43.096315Z"},"trusted":true},"outputs":[],"source":["test_folder_path = '/kaggle/input/tlvmc-parkinsons-freezing-gait-prediction/test/tdcsfog/'\n","test_files = [file for file in os.listdir(test_folder_path)]\n","test_dataset = SegmentedDataset(test_folder_path, test_files, is_train=False)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","trainer.test(model, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-06-09T10:29:43.099113Z","iopub.status.idle":"2023-06-09T10:29:43.09983Z","shell.execute_reply":"2023-06-09T10:29:43.099524Z","shell.execute_reply.started":"2023-06-09T10:29:43.099493Z"},"trusted":true},"outputs":[],"source":["result = pd.DataFrame()\n","for batch in model.output:\n","    preds = batch[0].cpu().numpy()\n","    ids = batch[1]\n","    times = batch[2].cpu().numpy()\n","    for i, time, pred in zip(ids, times, preds):\n","        id_time = [f'{i}_{t}' for t in time]\n","        tmp = pd.DataFrame(pred[:,:3], index=id_time).loc[[t != -1 for t in time]]\n","        result = pd.concat([result, tmp])\n","        \n","result.columns = ['StartHesitation', 'Turn', 'Walking']\n","\n","submission = submission.set_index('Id')\n","submission.update(result)\n","submission.reset_index().to_csv('submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":5677426,"sourceId":41880,"sourceType":"competition"}],"dockerImageVersionId":30497,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
